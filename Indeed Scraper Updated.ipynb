{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to update the Webscraping Indeed Notebook to Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Calls\n",
    "import requests\n",
    "# Parse HTML\n",
    "import bs4\n",
    "# Handle Dataframes (excel data)\n",
    "import pandas as pd\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching and Cleaning Indeed Search Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indeed_scraper import search_indeed, clean_data, save_data, posting_scraper\n",
    "from utils import save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./indeed_searches/machine+learning-2018_11_25-165725\n"
     ]
    }
   ],
   "source": [
    "# Fetch Page Information for Indeed Search\n",
    "# TODO: Add url encoding so you don't have to do it manually here.\n",
    "query = \"machine+learning\"\n",
    "cities = [\"New+York%2C+NY\", \"San+Francisco%2C+CA\", \"Boston%2C+MA\", \"Greenwich%2C+CT\"]\n",
    "max_results_per_city = 100\n",
    "null_value = \"NA\"\n",
    "\n",
    "df = search_indeed(query, cities, max_results_per_city, null_value)\n",
    "df = clean_data(df)\n",
    "filename = save_data(df, query, path='./indeed_searches/')\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Per Posting Page (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting one posting worth of data\n",
    "# url = data.loc[:, 'url'].values[0]\n",
    "# html = requests.get(url).text\n",
    "# soups = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "# # Print out job description as one srting\n",
    "# main_content = soups.find('div', {'class': \"jobsearch-JobComponent icl-u-xs-mt--sm jobsearch-JobComponent-bottomDivider\"})\n",
    "# job_description = soups.find('div', {'class': \"jobsearch-JobComponent-description icl-u-xs-mt--md\"})\n",
    "# # job_description.get_text(\"  \", strip=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print out job description as one srting\n",
    "# main_content = soups.find('div', {'class': \"jobsearch-JobComponent icl-u-xs-mt--sm jobsearch-JobComponent-bottomDivider\"})\n",
    "# job_description = soups.find('div', {'class': \"jobsearch-JobComponent-description icl-u-xs-mt--md\"})\n",
    "# # job_description.get_text(\"  \", strip=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Data Per Posting Page (WIP)\n",
    "data = pd.read_csv(f\"{filename}.csv\", index_col=0)\n",
    "desc_dataframe = posting_scraper(data, filename)\n",
    "data['desc'] = desc_dataframe['desc']\n",
    "\n",
    "data.to_csv(f'{filename}.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer on File\n",
    "Seeing most popular words in job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"machine+learning-2018_11_24-185749\"\n",
    "job_info = pd.read_csv(f'{filename}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "for txtfile in job_info['desc']:\n",
    "    with open(txtfile, 'r', encoding='utf-8') as the_file:\n",
    "        descriptions.append(the_file.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "def get_top_n_words(corpus, stop_words=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    sum_words = X.sum(axis=0).tolist()[0]\n",
    "    words_freq = zip(feature_names, sum_words)\n",
    "    words_freq =sorted(words_freq, key = lambda x: -x[1])\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from more_stop_words import more_stop_words\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union(more_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(descriptions)\n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "X = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Summing words along columns to find total amount of occurences per word.\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "sum_words = X.sum(axis=0).tolist()[0]\n",
    "words_freq = zip(feature_names, sum_words)\n",
    "words_freq =sorted(words_freq, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_map = [word for word in words_freq if word[1] > 0]\n",
    "# print(words_freq[:20])\n",
    "# final_map[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF Transformer and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X_tf = transformer.fit_transform(X)\n",
    "# print(X_tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=20, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "# Using 8 buckets, should use differnt amount.\n",
    "km = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X_tf)\n",
    "\n",
    "# k means determine k\n",
    "# import time\n",
    "# # 100-150 -> .70-.58\n",
    "# distortions = []\n",
    "# K = range(1, 394)\n",
    "# for k in K:\n",
    "#     start = time.time()\n",
    "#     X_tf_arr = X_tf.toarray()\n",
    "#     kmeanModel = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "#     kmeanModel.fit(X_tf_arr)\n",
    "#     distortions.append(sum(np.min(cdist(X_tf_arr, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_tf_arr.shape[0])\n",
    "#     end = time.time()\n",
    "#     print(k, end - start)\n",
    "# Plot the elbow\n",
    "# plt.plot(K, distortions, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method showing the optimal k')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the elbow\n",
    "# start = 315\n",
    "# step = 15\n",
    "# plt.plot(K[start::step], distortions[start::step], 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method showing the optimal k')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: trading investment financial quantitative markets statistical strategies quant firm electronic\n",
      "Cluster 1: ai processing natural language deep artificial neural python java recognition\n",
      "Cluster 2: twitter aws relevance recommendation francisco reviews distributed scala ipsy knowledgeable\n",
      "Cluster 3: clinical computational biology regeneron genetic biological drug genetics datasets disease\n",
      "Cluster 4: applicant privacy regional makes arrow linked browser confirm submitting agree\n",
      "Cluster 5: analytics modeling statistical diabetes analytic hockey ascensia scientist consultant analytical\n",
      "Cluster 6: ibm quantum font analytics chief analytical language eo prof power\n",
      "Cluster 7: marketing riskmatch analytics honor mapping tableau sql visualization external attribution\n",
      "Cluster 8: deep hardware accelerator publication temboo frameworks caffe tensorflow keras dolby\n",
      "Cluster 9: language speech natural processing alexa nlp scientists applied modeling recognition\n",
      "Cluster 10: students prime pathrise teaching curriculum instructors instructor classes guidelines classroom\n",
      "Cluster 11: 3d unity graphics robot creations ml rl shenzhen beijing meshes\n",
      "Cluster 12: ca sales hand francisco youtube wa york architectures frameworks legal\n",
      "Cluster 13: boston python 180 tensorflow nlp healthcare neural numpy ml scipy\n",
      "Cluster 14: bose pdf gov dol ofccp clover wellness http posters regs\n",
      "Cluster 15: analog str signal raytheon networking adi electronic security bbn electrical\n",
      "Cluster 16: harvard scientist boston yelp experimental club nift ma deep local\n",
      "Cluster 17: credit strategy affirm risk consumer strategies reasonable time agile months\n",
      "Cluster 18: agile iris pluralsight music personalized stack dev fundamentals designer continuous\n",
      "Cluster 19: ml ai adobe hadoop java spark deep hive mapr scala\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = feature_names\n",
    "for i in range(len(order_centroids)):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (29,)\n",
      "1 (22,)\n",
      "2 (18,)\n",
      "3 (28,)\n",
      "4 (9,)\n",
      "5 (45,)\n",
      "6 (10,)\n",
      "7 (22,)\n",
      "8 (15,)\n",
      "9 (38,)\n",
      "10 (12,)\n",
      "11 (6,)\n",
      "12 (19,)\n",
      "13 (21,)\n",
      "14 (5,)\n",
      "15 (10,)\n",
      "16 (8,)\n",
      "17 (14,)\n",
      "18 (7,)\n",
      "19 (56,)\n"
     ]
    }
   ],
   "source": [
    "# Print out all posting in that cluster\n",
    "import numpy as np\n",
    "\n",
    "def cluster_index(values, searchval):\n",
    "    return np.where(values == searchval)[0]\n",
    "for i in range(len(order_centroids)):\n",
    "    print(i, cluster_index(km.labels_, i).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# filtered_data = data.loc[ cluster_index(km.labels_, 8), ['company', 'job_title', 'url', 'location']]\n",
    "# filtered_data = filtered_data[filtered_data['company'].str.contains('Adobe')]\n",
    "\n",
    "# filtered_data.to_csv('most_promising_k_10.csv', sep=',', encoding='utf-8')\n",
    "for i in range(len(order_centroids)):\n",
    "    filtered_data = data.loc[ cluster_index(km.labels_, i), ['company', 'job_title', 'url', 'location']]\n",
    "    filtered_data['company'] = filtered_data['company'].str.strip()\n",
    "    filtered_data.to_csv(f'cluster_{i}_k_20.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps are to sort these words into categories (job, technologies, companies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
