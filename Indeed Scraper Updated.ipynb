{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to update the Webscraping Indeed Notebook to Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Calls\n",
    "import requests\n",
    "# Parse HTML\n",
    "import bs4\n",
    "# Handle Dataframes (excel data)\n",
    "import pandas as pd\n",
    "# Plotting library\n",
    "# import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching and Cleaning Indeed Search Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indeed_scraper import search_indeed, clean_data, save_data, posting_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\pandas\\core\\generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "E:\\Users\\Jonathan\\Coding\\Python\\career-posting-scraper\\indeed_scraper.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df_copy['url'] = \"h\" + \"ttps://www.indeed.com/viewjob?jk=\" + df_copy['url']\n"
     ]
    }
   ],
   "source": [
    "# Fetch Page Information for Indeed Search\n",
    "query = \"machine+learning\"\n",
    "cities = [\"New+York%2C+NY\"]\n",
    "max_results_per_city = 100\n",
    "null_value = \"NA\"\n",
    "\n",
    "df = search_indeed(query, cities, max_results_per_city, null_value)\n",
    "df = clean_data(df)\n",
    "filename = save_data(df, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Per Posting Page (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting one posting worth of data\n",
    "# url = data.loc[:, 'url'].values[0]\n",
    "# html = requests.get(url).text\n",
    "# soups = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "# # Print out job description as one srting\n",
    "# main_content = soups.find('div', {'class': \"jobsearch-JobComponent icl-u-xs-mt--sm jobsearch-JobComponent-bottomDivider\"})\n",
    "# job_description = soups.find('div', {'class': \"jobsearch-JobComponent-description icl-u-xs-mt--md\"})\n",
    "# # job_description.get_text(\"  \", strip=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print out job description as one srting\n",
    "# main_content = soups.find('div', {'class': \"jobsearch-JobComponent icl-u-xs-mt--sm jobsearch-JobComponent-bottomDivider\"})\n",
    "# job_description = soups.find('div', {'class': \"jobsearch-JobComponent-description icl-u-xs-mt--md\"})\n",
    "# # job_description.get_text(\"  \", strip=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Data Per Posting Page (WIP)\n",
    "data = pd.read_csv(f\"{filename}.csv\", index_col=0)\n",
    "desc_dataframe = posting_scraper(data, filename)\n",
    "data['desc'] = desc_dataframe['desc']\n",
    "\n",
    "data.to_csv(f'{filename}.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer on File\n",
    "Seeing most popular words in job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"machine+learning-2018_11_24-185749\"\n",
    "job_info = pd.read_csv(f'{filename}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "for txtfile in job_info['desc']:\n",
    "    with open(txtfile, 'r', encoding='utf-8') as the_file:\n",
    "        descriptions.append(the_file.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "def get_top_n_words(corpus, stop_words=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    sum_words = X.sum(axis=0).tolist()[0]\n",
    "    words_freq = zip(feature_names, sum_words)\n",
    "    words_freq =sorted(words_freq, key = lambda x: -x[1])\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from more_stop_words import more_stop_words\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union(more_stop_words)\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(descriptions)\n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "X = vectorizer.fit_transform(descriptions)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "sum_words = X.sum(axis=0).tolist()[0]\n",
    "words_freq = zip(feature_names, sum_words)\n",
    "words_freq =sorted(words_freq, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('research', 175),\n",
       " ('models', 111),\n",
       " ('ml', 97),\n",
       " ('ai', 91),\n",
       " ('deep', 91),\n",
       " ('python', 90),\n",
       " ('language', 79),\n",
       " ('analytics', 72),\n",
       " ('java', 70),\n",
       " ('york', 69),\n",
       " ('natural', 55),\n",
       " ('financial', 49),\n",
       " ('applied', 45),\n",
       " ('statistics', 43),\n",
       " ('quantitative', 42),\n",
       " ('scientists', 42),\n",
       " ('modeling', 41),\n",
       " ('nlp', 38),\n",
       " ('scientist', 38),\n",
       " ('artificial', 36),\n",
       " ('methods', 35),\n",
       " ('sales', 33),\n",
       " ('tensorflow', 33),\n",
       " ('aws', 31),\n",
       " ('phd', 31),\n",
       " ('optimization', 30),\n",
       " ('training', 30),\n",
       " ('amazon', 29),\n",
       " ('conferences', 29),\n",
       " ('hadoop', 29),\n",
       " ('statistical', 29),\n",
       " ('distributed', 28),\n",
       " ('vision', 28),\n",
       " ('analytical', 27),\n",
       " ('spark', 27),\n",
       " ('education', 26),\n",
       " ('frameworks', 26),\n",
       " ('model', 26),\n",
       " ('scala', 26),\n",
       " ('deploying', 25),\n",
       " ('master', 25),\n",
       " ('startup', 25),\n",
       " ('feature', 24),\n",
       " ('group', 24),\n",
       " ('mining', 24),\n",
       " ('senior', 24),\n",
       " ('speech', 24),\n",
       " ('state', 24),\n",
       " ('advanced', 23),\n",
       " ('agile', 23),\n",
       " ('ideas', 23),\n",
       " ('multiple', 23),\n",
       " ('proven', 23),\n",
       " ('scalable', 23),\n",
       " ('text', 23),\n",
       " ('marketing', 22),\n",
       " ('predictive', 22),\n",
       " ('recognition', 22),\n",
       " ('recommendation', 22),\n",
       " ('health', 21),\n",
       " ('investment', 21),\n",
       " ('mathematics', 21),\n",
       " ('academic', 20),\n",
       " ('datasets', 20),\n",
       " ('markets', 20),\n",
       " ('classification', 19),\n",
       " ('expert', 19),\n",
       " ('internship', 19),\n",
       " ('sets', 19),\n",
       " ('strategies', 19),\n",
       " ('strategy', 19),\n",
       " ('ca', 18),\n",
       " ('competitive', 18),\n",
       " ('computational', 18),\n",
       " ('cross', 18),\n",
       " ('legal', 18),\n",
       " ('platforms', 18),\n",
       " ('recommendations', 18),\n",
       " ('significant', 18),\n",
       " ('spotify', 18),\n",
       " ('art', 17),\n",
       " ('collaborative', 17),\n",
       " ('digital', 17),\n",
       " ('findmine', 17),\n",
       " ('identify', 17),\n",
       " ('mapr', 17),\n",
       " ('medical', 17),\n",
       " ('ms', 17),\n",
       " ('proficiency', 17),\n",
       " ('scientific', 17),\n",
       " ('security', 17),\n",
       " ('sql', 17),\n",
       " ('unique', 17),\n",
       " ('achieve', 16),\n",
       " ('bachelor', 16),\n",
       " ('bs', 16),\n",
       " ('changing', 16),\n",
       " ('comfortable', 16),\n",
       " ('connect', 16),\n",
       " ('consumer', 16),\n",
       " ('environments', 16),\n",
       " ('external', 16),\n",
       " ('sources', 16),\n",
       " ('stakeholders', 16),\n",
       " ('storage', 16),\n",
       " ('tech', 16),\n",
       " ('travel', 16),\n",
       " ('architectures', 15),\n",
       " ('experienced', 15),\n",
       " ('capital', 14),\n",
       " ('communicate', 14),\n",
       " ('contract', 14),\n",
       " ('deploy', 14),\n",
       " ('don', 14),\n",
       " ('english', 14),\n",
       " ('exciting', 14),\n",
       " ('law', 14),\n",
       " ('manager', 14),\n",
       " ('music', 14),\n",
       " ('network', 14),\n",
       " ('networks', 14),\n",
       " ('nyu', 14),\n",
       " ('papers', 14),\n",
       " ('ph', 14),\n",
       " ('pipelines', 14),\n",
       " ('researchers', 14),\n",
       " ('retail', 14),\n",
       " ('similar', 14),\n",
       " ('solid', 14),\n",
       " ('term', 14),\n",
       " ('way', 14),\n",
       " ('approach', 13),\n",
       " ('backgrounds', 13),\n",
       " ('banking', 13),\n",
       " ('built', 13),\n",
       " ('decision', 13),\n",
       " ('detection', 13),\n",
       " ('dynamic', 13),\n",
       " ('familiar', 13),\n",
       " ('finance', 13),\n",
       " ('includes', 13),\n",
       " ('insurance', 13),\n",
       " ('letter', 13),\n",
       " ('maintain', 13),\n",
       " ('market', 13),\n",
       " ('metrics', 13),\n",
       " ('news', 13),\n",
       " ('offerings', 13),\n",
       " ('relationships', 13),\n",
       " ('temboo', 13),\n",
       " ('unstructured', 13),\n",
       " ('values', 13),\n",
       " ('ancestry', 12),\n",
       " ('audiences', 12),\n",
       " ('automated', 12),\n",
       " ('come', 12),\n",
       " ('commercial', 12),\n",
       " ('creation', 12),\n",
       " ('cv', 12),\n",
       " ('daily', 12),\n",
       " ('define', 12),\n",
       " ('discipline', 12),\n",
       " ('entrepreneurial', 12),\n",
       " ('flexible', 12),\n",
       " ('gartner', 12),\n",
       " ('identifying', 12),\n",
       " ('independently', 12),\n",
       " ('initiatives', 12),\n",
       " ('latest', 12),\n",
       " ('positive', 12),\n",
       " ('protected', 12),\n",
       " ('prototype', 12),\n",
       " ('providing', 12),\n",
       " ('scikit', 12),\n",
       " ('train', 12),\n",
       " ('trends', 12),\n",
       " ('visualization', 12),\n",
       " ('account', 11),\n",
       " ('actionable', 11),\n",
       " ('ads', 11),\n",
       " ('bloomberg', 11),\n",
       " ('cloudera', 11),\n",
       " ('constantly', 11),\n",
       " ('date', 11),\n",
       " ('dedicated', 11),\n",
       " ('effectively', 11),\n",
       " ('enable', 11),\n",
       " ('excited', 11),\n",
       " ('execute', 11),\n",
       " ('expected', 11),\n",
       " ('foundation', 11),\n",
       " ('graduate', 11),\n",
       " ('https', 11),\n",
       " ('improving', 11),\n",
       " ('journals', 11),\n",
       " ('matlab', 11),\n",
       " ('multi', 11),\n",
       " ('neural', 11),\n",
       " ('novel', 11),\n",
       " ('order', 11),\n",
       " ('organizations', 11),\n",
       " ('place', 11),\n",
       " ('preferably', 11),\n",
       " ('publications', 11),\n",
       " ('reliability', 11),\n",
       " ('serve', 11),\n",
       " ('share', 11),\n",
       " ('specific', 11),\n",
       " ('structured', 11),\n",
       " ('trading', 11),\n",
       " ('voice', 11)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_map = [word for word in words_freq if word[1] > 10]\n",
    "# print(words_freq[:])\n",
    "final_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
